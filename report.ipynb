{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f701128",
   "metadata": {},
   "source": [
    "<div style=\"\n",
    "    border: 3px solid white;\n",
    "    border-radius: 16px;\n",
    "    padding: 10px;\n",
    "    background-color: #4F2683;\n",
    "    color: white;\n",
    "    max-width: 1300px;\n",
    "    margin: auto;\n",
    "\">\n",
    "\n",
    "<p align=\"center\" style=\"margin: 0;\">\n",
    "  <img src=\"WesternEng.png\" \n",
    "       alt=\"Western University Logo\" width=\"200\">\n",
    "</p>\n",
    "\n",
    "<h2 align=\"center\" style=\"color:white; margin: 6px 0 2px 0;\">Western University</h2>\n",
    "<h3 align=\"center\" style=\"color:white; margin: 2px 0;\">Faculty of Engineering</h3>\n",
    "<h4 align=\"center\" style=\"color:white; margin: 2px 0;\">Department of Electrical & Computer Engineering</h4>\n",
    "\n",
    "<hr style=\"border: 1px solid white; margin: 8px 0;\">\n",
    "\n",
    "<div align=\"center\" style=\"margin: 0; padding: 0; line-height: 1.3;\">\n",
    "\n",
    "**AISE-3350 — Cyber-Physical Systems Theory**  \n",
    "Instructor: **Elvis Chen**  \n",
    "Date: *[Insert Date]*  \n",
    "Group Members: *Jonathan Das,*  \n",
    "\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: 1px solid white; margin: 8px 0;\">\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bae09c",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "***\n",
    "##### [1. Introduction](#introduction)\n",
    "##### [2. Methods](#methods)\n",
    "-   [Strategic Framework](#strategic-framework-and-loss-minimization-logic)\n",
    "-   [Strategy-Valuation Adjustment](#strategy-valuation-adjustment)\n",
    "-   [Hand Detection and Gesture Classification](#hand-detection-and-gesture-classification)\n",
    "-   [Player Segmentation Logic](#player-segmentation-logic)\n",
    "-   [System Integration](#system-integration)\n",
    "##### [3. Machine Learning Model](#machine-learning-model)\n",
    "-   [Dataset Pre-Processing](#dataset-pre-processing)\n",
    "-   [Model Training](#model-training)\n",
    "-   [Model Performance on Static Dataset](#model-performance-on-static-dataset)\n",
    "##### [4. System Integration - Live Usage and Testing](#live-usage)\n",
    "##### [5. Broader Impacts & Ethics](#broader-impacts-and-ethical-considerations)\n",
    "\n",
    "\n",
    "\n",
    "<br><br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7437a760",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909b67d0",
   "metadata": {},
   "source": [
    "Cyber-physical systems (CPS) consist of three entities, sensing, computing, and actuating working together to enable real-time processing and response. This project integrates computer vision and corresponding game theory to develop a CPS suited to play Rock-Paper-Scissors-Minus-One (RPS-1).\n",
    "\n",
    "The more commonly known variant Rock-Paper-Scissors (RPS) offers a simple pairwise competition between two players. The “Minus-One” (RPS-1) variant introduces complexity to this decision space by introducing one additional hand per player. Players must present both of their hands and then remove one, with the remaining pair being the decider. This addition makes the game a two-stage, simultaneous, zero-sum game.  \n",
    "\n",
    "The game of RPS-1 was chosen as it requires the project to apply all pillars of CPS. Sensing using Computer Vision (CV) and Machine Learning (ML) to detect the player’s actions. Computation to identify gestures and compute the game theory principles and payoff matrices. Lastly, a decision recommending what the player should do to maximize their chances of winning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a62312a",
   "metadata": {},
   "source": [
    "## **Methods**\n",
    "---\n",
    "## Overview\n",
    "\n",
    "The objective of the CPS is (1) to identify the hand shapes being presented by each player for stage one and (2) to determine the optimal action to win the game through a game-theoretical analysis. The workflow consists of three major components: **hand detection and classification**, **enumeration of all possible outcomes**, and **strategy selection**.\n",
    "\n",
    "The system first detects up to four hands simultaneously and classifies each as *Rock*, *Paper*, or *Scissors*. From these detected shapes, all feasible hand-combinations are computed using Cartesian enumeration. A payoff analysis is then applied to determine which move Player 1 should remove in Stage Two to minimize expected loss and maximize strategic advantage. This modular structure isolates perception from decision-making, allowing both components to be improved independently.\n",
    "\n",
    "\n",
    "---\n",
    "## **Strategic Framework and Loss Minimization Logic**\n",
    "\n",
    "#### **Decision and Outcome Representation**\n",
    "\n",
    "\n",
    "To formally model the restricted RPS-1 game, a payoff function is defined following standard RPS rules. For any pair of moves \\(p_1\\) (Player 1) and \\(p_2\\) (Player 2), the utility assigned to Player 1 is:\n",
    "\n",
    "$$\n",
    "U(p_1,p_2)=\n",
    "\\begin{cases}\n",
    "+1 & \\text{if } p_1 \\text{ beats } p_2, \\\\\n",
    "0 & \\text{if } p_1 = p_2, \\\\\n",
    "-1 & \\text{if } p_2 \\text{ beats } p_1.\n",
    "\\end{cases}\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "The system computes the Cartesian product of available actions:\n",
    "\n",
    "$$\n",
    "\\Omega = P_1 \\times P_2\n",
    "\\tag{2}\n",
    "$$\n",
    "\n",
    "where \\(P_1\\) and \\(P_2\\) are the move subsets available to each player. This generates all possible Stage Two interactions and their payoffs.\n",
    "\n",
    "#### **Outcome Enumeration and Expected Statistics**\n",
    "\n",
    "For each move \\(m \\in P_1\\), the system counts the number of wins, losses, and draws:\n",
    "\n",
    "$$\n",
    "W_m = |\\{(m,p_2) \\in \\Omega : U(m,p_2)=+1\\}|, \\tag{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_m = |\\{(m,p_2) \\in \\Omega : U(m,p_2)=-1\\}|, \\tag{4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "D_m = |\\{(m,p_2) \\in \\Omega : U(m,p_2)=0\\}|. \\tag{5}\n",
    "$$\n",
    "\n",
    "From this, a basic expected value (risk-neutral utility) is computed:\n",
    "\n",
    "$$\n",
    "EV(m) = \\frac{W_m - L_m}{|\\Omega|}.\n",
    "\\tag{6}\n",
    "$$\n",
    "\n",
    "Equation (6) quantifies the relative strength or weakness of each move in the restricted subgame.\n",
    "\n",
    "#### **Loss Minimization Logic**\n",
    "\n",
    "If all outcomes in \\( \\Omega \\) satisfy:\n",
    "\n",
    "$$\n",
    "U(p_1,p_2) \\ge 0 \\quad \\forall (p_1,p_2)\\in\\Omega,\n",
    "\\tag{7}\n",
    "$$\n",
    "\n",
    "then no losing scenario exists, and the system concludes:\n",
    "\n",
    "**All roads lead to victory**.\n",
    "\n",
    "If losses exist, the system isolates the losing moves:\n",
    "\n",
    "$$\n",
    "L = \\{(p_1,p_2)\\in\\Omega : U(p_1,p_2) = -1\\}.\n",
    "\\tag{8}\n",
    "$$\n",
    "\n",
    "Each losing move is grouped by the Player 1 action that created it:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(m) = |\\{(m,p_2)\\in L\\}|.\n",
    "\\tag{9}\n",
    "$$\n",
    "\n",
    "If one move \\(m\\) accounts for all losses:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(m) = |L|,\n",
    "\\tag{10}\n",
    "$$\n",
    "\n",
    "the system immediately recommends dropping that move.\n",
    "\n",
    "---\n",
    "\n",
    "## **Strategy-Valuation Adjustment**\n",
    "\n",
    "To incorporate strategic preferences, the system defines a **strategy-dependent utility transformation**, modifying the payoff of each outcome according to Player 2's most likely reply.\n",
    "\n",
    "#### **Opponent Move Likelihood Model**\n",
    "\n",
    "The likelihood score for each opponent move is computed from:\n",
    "\n",
    "$$\n",
    "v(p_2) = |\\{(p_1,p_2)\\in L\\}| - |\\{(p_1,p_2)\\in W\\}|,\n",
    "\\tag{11}\n",
    "$$\n",
    "\n",
    "which emphasizes actions by Player 2 that historically cause Player 1 to lose.\n",
    "\n",
    "The predicted reply is:\n",
    "\n",
    "$$\n",
    "p_2^* = \\arg\\max_{p_2\\in P_2} v(p_2).\n",
    "\\tag{12}\n",
    "$$\n",
    "\n",
    "#### **Strategy Profile Weighting**\n",
    "\n",
    "Each strategy assigns six weights:\n",
    "\n",
    "$$\n",
    "[w_s, \\; l_s, \\; d_s, \\; w_o, \\; l_o, \\; d_o],\n",
    "\\tag{13}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- \\(w_s, l_s, d_s\\) apply when player 2 plays the **predicted move** \\(p_2^*\\),\n",
    "- \\(w_o, l_o, d_o\\) apply when player 2 chooses **other moves**.\n",
    "\n",
    "The transformed utility for each outcome \\((p_1,p_2,U)\\) becomes:\n",
    "\n",
    "If \\(p_2 = p_2^*\\):\n",
    "\n",
    "$$\n",
    "U'(p_1,p_2)=\n",
    "\\begin{cases}\n",
    "w_s & U=+1, \\\\\n",
    "-l_s & U=-1, \\\\\n",
    "d_s & U=0,\n",
    "\\end{cases}\n",
    "\\tag{14}\n",
    "$$\n",
    "\n",
    "Else:\n",
    "\n",
    "$$\n",
    "U'(p_1,p_2)=\n",
    "\\begin{cases}\n",
    "w_o & U=+1, \\\\\n",
    "-l_o & U=-1, \\\\\n",
    "d_o & U=0.\n",
    "\\end{cases}\n",
    "\\tag{15}\n",
    "$$\n",
    "\n",
    "#### **Weighted Move Evaluation**\n",
    "\n",
    "For each Player 1 move:\n",
    "\n",
    "$$\n",
    "\\text{WeightedValue}(m)\n",
    " = \\sum_{\\substack{(p_1,p_2)\\in\\Omega\\\\p_1=m}} U'(p_1,p_2).\n",
    "\\tag{16}\n",
    "$$\n",
    "\n",
    "The **recommended move to drop** is the one with minimal value:\n",
    "\n",
    "$$\n",
    "m_{\\text{drop}} = \\arg\\min_{m\\in P_1} \\text{WeightedValue}(m).\n",
    "\\tag{17}\n",
    "$$\n",
    "\n",
    "This ensures Player 1 removes the move most detrimental under the selected strategy profile.\n",
    "\n",
    "---\n",
    "\n",
    "## **Hand Detection and Gesture Classification**\n",
    "\n",
    "Three gesture-recognition methods were evaluated:\n",
    "\n",
    "1. **Mediapipe Landmark Classifier** – fast, rule-based, no training required - Ultimatley rejected due to its non-generalizability .\n",
    "2. **Mediapipe Gesture Recognition Model** – rejected due to poor robustness to lighting and gesture ambiguity.\n",
    "3. **Custom YOLOv11 Gesture Detector** – trained specifically on RPS gestures for real-time accuracy.\n",
    "\n",
    "YOLOv11 was chosen due to high-speed multi-object detection and domain-specific fine-tuning through:\n",
    "\n",
    "- Image annotation via custom scripts,\n",
    "- Automatic label generation and dataset structuring,\n",
    "- Controlled training of a YOLOv11 backbone for 80 epochs.\n",
    "\n",
    "The resulting model outputs symbolic labels suitable for the decision-making engine.\n",
    "\n",
    "---\n",
    "\n",
    "## **Player Segmentation Logic**\n",
    "\n",
    "YOLO detection uses **spatial partitioning**:\n",
    "\n",
    "- Upper frame regions → Player 2\n",
    "- Lower frame regions → Player 1\n",
    "\n",
    "This allows clean assignment of gestures even with simultaneous detections, assuming a fixed camera orientation.\n",
    "\n",
    "---\n",
    "\n",
    "## **System Integration**\n",
    "\n",
    "After gesture detection, moves are passed to the decision engine, where all combinations of Stage Two interactions are evaluated using equations (1)–(17). The computed recommendation is then overlayed on the live video feed using OpenCV, enabling real-time feedback.\n",
    "\n",
    "Modular separation of:\n",
    "\n",
    "- Detection,\n",
    "- Classification,\n",
    "- Segmentation,\n",
    "- Strategy evaluation,\n",
    "\n",
    "ensures ease of debugging, reproducibility, and adherence to CPS design principles.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aad963",
   "metadata": {},
   "source": [
    "## **Machine Learning Model**\n",
    "---\n",
    "\n",
    "## **Dataset Pre-Processing**\n",
    "[View Pre-Processing Step 1](modelTraining\\imageannotation.py)\\\n",
    "[View Pre-Processing Step 2](modelTraining\\datasplit.py)\n",
    "\n",
    "\n",
    "In our implementation, we selected **YOLOv11** because it proved to be the most robust model for identifying and labeling up to four hands in a single frame. To make YOLOv11 suitable for our RPS-1 application, we trained it on numerous hand-gesture images consisting of the three target classes: **Rock**, **Paper**, and **Scissors**. Due to the similarity in publicly available datasets on Kaggle, three datasets were merged to reduce the chance of overfitting and increase generalizability.\n",
    "\n",
    "To account for variations in image formats, `imageannotation.py` automatically handles JPG, JPEG, and PNG files. A pretrained YOLOv11 model is used to localize the hand in each image and draw a bounding box around it. These bounding boxes provide clear supervision signals during training, ensuring the model focuses on the gesture-performing region.\n",
    "\n",
    "Following annotation, `datasplit.py` divides the dataset into an **80% training** and **20% validation** split, randomly shuffling samples before placement into new directories. Because YOLOv11 requires a YAML configuration file, `datasplit.py` generates this file automatically for training via `rps_model.py`.\n",
    "\n",
    "---\n",
    "## **Model Training**  \n",
    " [View Training Code](modelTraining\\rps_model.py)\n",
    "\n",
    "\n",
    "The training pipeline uses a pretrained YOLOv11s model as its foundation. Starting from a pretrained backbone significantly accelerates convergence because the model already understands general visual patterns such as edges, shapes, and textures. This allows the network to focus on learning the distinctions between the three gesture classes, even with a limited dataset.\n",
    "\n",
    "A dataset YAML file defines the training and validation directories as well as the class labels. During the training process, YOLOv11 automatically handles dataloading, batch creation, and optimization, while continuously monitoring performance metrics such as Loss, Precision, Recall, and mean Average Precision (mAP).\n",
    "\n",
    "### **Data Augmentation Strategy**\n",
    "\n",
    "To ensure that the model generalizes well to real-world camera conditions, a comprehensive augmentation pipeline is applied during training. These augmentations include:\n",
    "\n",
    "- **HSV colour variation** to simulate different lighting environments  \n",
    "- **Rotations, translations, scaling, and shearing** to vary the spatial relationships of the hands  \n",
    "- **Perspective distortion** to represent off-angle camera viewpoints  \n",
    "- **Horizontal and vertical flips** to account for mirrored gestures  \n",
    "- **Mosaic augmentation** to expose the model to multiple contexts simultaneously  \n",
    "- **MixUp blending** to regularize decision boundaries  \n",
    "\n",
    "This augmentation strategy minimizes overfitting and enhances robustness against noise, motion, and inconsistent hand positioning. Furthermore, the use of mosic augmentation specifically allows the model to learn from images containing more than one hand which is important to consider given the task at hand and the limitations of the dataset.\n",
    "\n",
    "### **Hyperparameter Configuration**\n",
    "\n",
    "Training is performed for 80 epochs at a resolution of 640×640 with a batch size of 16. A cosine learning-rate scheduler is used to smooth the convergence behaviour over time, and early stopping is enabled to prevent unnecessary computation if validation performance plateaus. These hyperparameters offer a practical balance between training speed and detection accuracy.\n",
    "\n",
    "### **Model Export and Integration**\n",
    "\n",
    "Upon completion of training, YOLOv11 automatically exports the best checkpoint—based on validation results—into a dedicated project folder. This exported weight file becomes the inference model used in the real-time hand-detection system of the CPS. By integrating this optimized model into the perception pipeline, the system achieves:\n",
    "\n",
    "- Fast and accurate gesture detection  \n",
    "- Reliable multi-hand tracking  \n",
    "- Robust performance in general indoor lighting  \n",
    "- Smooth interaction with the downstream decision-making engine  \n",
    "\n",
    "### **Significance to the CPS Project**\n",
    "\n",
    "The YOLOv11 training pipeline is an essential component of the RPS-1 system. It transforms raw video input into symbolic gesture predictions, enabling the game decision engine to operate on high-confidence inputs. The pipeline demonstrates how modern computer-vision models can be tailored effectively to domain-specific tasks, even with mixed datasets and limited hardware resources.\n",
    "\n",
    "Overall, this training process provides the foundation for a responsive, accurate, and highly adaptable gesture-recognition subsystem within the broader CPS architecture.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **Model Performance on Static Dataset**\n",
    "\n",
    "### **F1-Confidence Curve** *(Figure 1)*\n",
    "\n",
    "<img src=\"rps_yolo11/F1_curve.png\" width=\"450\">\n",
    "\n",
    "Figure 1 illustrates the F1-confidence curve for the YOLOv11 gesture classifier. The model stabilizes above an **F1 score of 0.95** between confidence intervals of **0.2 and 0.8**, showing excellent robustness. The optimal performance appears at a confidence threshold of **approximately 0.4**, where the model reaches an F1 score of **~0.98**.\n",
    "\n",
    "The **rock** gesture consistently shows the lowest F1 score, though still high enough to avoid significant gameplay disruption.\n",
    "\n",
    "---\n",
    "\n",
    "### **Precision-Recall Curve** *(Figure 2)*\n",
    "\n",
    "<img src=\"rps_yolo11/PR_curve.png\" width=\"450\">\n",
    "\n",
    "Figure 2 presents the Precision–Recall curve demonstrating the model's reliability across the full recall spectrum. The model achieves **mAP = 0.984 at IoU = 0.5**, highlighting excellent predictive performance. Paper and scissors gestures achieve AP values of **0.993** and **0.991**, respectively. Although rock underperforms relative to these two, its AP of **0.967** demonstrates strong detectability.\n",
    "\n",
    "---\n",
    "\n",
    "## **Confusion Matrices** *(Figures 3 and 4)*\n",
    "\n",
    "### **Figure 3 — Raw Confusion Matrix**\n",
    "\n",
    "<img src=\"rps_yolo11/confusion_matrix.png\" width=\"450\">\n",
    "\n",
    "### **Figure 4 — Normalized Confusion Matrix**\n",
    "\n",
    "<img src=\"rps_yolo11/confusion_matrix_normalized.png\" width=\"450\">\n",
    "\n",
    "Figures 3 and 4 further validate classification performance. The raw confusion matrix reveals that the **rock** gesture experiences the highest misclassification count (~40 errors). Conversely, **paper** and **scissors** demonstrate far fewer errors (~10–15).\n",
    "\n",
    "Normalized results indicate:\n",
    "\n",
    "- **Paper** accuracy: **1.00**\n",
    "- **Scissors** accuracy: **0.99**\n",
    "- **Rock** accuracy: **0.97**\n",
    "\n",
    "These high values confirm the consistency and reliability of the YOLOv11 detector.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d5c24a",
   "metadata": {},
   "source": [
    "# **Live Usage**\n",
    "[View Computer Vision Script](RPSCamera.py)\\\n",
    "[View Game Logic](RPSLogic.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c4d3f",
   "metadata": {},
   "source": [
    "\n",
    "## **Broader Impacts and Ethical Considerations**\n",
    "\n",
    "Although this project may initially appear as a playful gesture-based game, its underlying technologies—computer vision, gesture recognition, real-time decision systems, and human–machine interaction—have broad implications extending well beyond the Rock–Paper–Scissors context.\n",
    "\n",
    "Gesture-recognition interfaces can transform how users interact with machines by enabling **hands-free**, **intuitive**, and **accessible** control mechanisms. This has applications in:\n",
    "\n",
    "- Operating rooms (sterile, hands-free environments)  \n",
    "- Assistive technologies for individuals with limited mobility  \n",
    "- Robotics and industrial automation  \n",
    "- Retail systems (touchless kiosks)  \n",
    "- Educational tools leveraging physical interaction  \n",
    "\n",
    "By relying on **low-cost webcam hardware** and **open-source ML frameworks**, this project demonstrates that advanced computer vision systems can be developed without expensive infrastructure, lowering adoption barriers for small companies and research groups.\n",
    "\n",
    "### **Economic and Ethical Considerations**\n",
    "\n",
    "Widespread adoption of AI-driven, gesture-based systems introduces economic and ethical challenges:\n",
    "\n",
    "- **Workforce displacement** due to automation  \n",
    "- The need for **upskilling** to support AI systems  \n",
    "- **Privacy concerns** regarding image capture  \n",
    "- **Bias risks** if training datasets are not diverse  \n",
    "- The need for transparent, explainable AI in public-use interfaces  \n",
    "\n",
    "Addressing these challenges is essential for responsible implementation and sustaining public trust in vision-based AI systems.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
